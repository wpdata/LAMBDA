#!/usr/bin/env python3
"""
LAMBDA ç³»ç»ŸéªŒè¯è¿è¡Œè„šæœ¬
æä¾›å¤šç§éªŒè¯æ¨¡å¼å’Œé€‰é¡¹
"""

import argparse
import sys
import os
from datetime import datetime
import json

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="LAMBDA å¯¹è¯å¼æ•°æ®åˆ†æç³»ç»ŸéªŒè¯å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_validation.py --mode quick                    # å¿«é€ŸéªŒè¯
  python run_validation.py --mode full                     # å®Œæ•´éªŒè¯
  python run_validation.py --mode full --config custom.yaml  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  python run_validation.py --mode report --input results/  # åŸºäºç°æœ‰ç»“æœç”ŸæˆæŠ¥å‘Š
        """
    )
    
    parser.add_argument(
        '--mode', 
        choices=['quick', 'full', 'report'],
        default='quick',
        help='éªŒè¯æ¨¡å¼: quick(å¿«é€ŸéªŒè¯), full(å®Œæ•´éªŒè¯), report(ä»…ç”ŸæˆæŠ¥å‘Š)'
    )
    
    parser.add_argument(
        '--config',
        default='validation_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: validation_config.yaml)'
    )
    
    parser.add_argument(
        '--output',
        default='validation_results',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: validation_results)'
    )
    
    parser.add_argument(
        '--input',
        help='è¾“å…¥ç»“æœç›®å½• (ä»…ç”¨äºreportæ¨¡å¼)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
    )
    
    parser.add_argument(
        '--no-charts',
        action='store_true',
        help='ä¸ç”Ÿæˆå›¾è¡¨'
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ LAMBDA ç³»ç»ŸéªŒè¯å·¥å…·")
    print("=" * 50)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)
    
    try:
        if args.mode == 'quick':
            run_quick_validation(args)
        elif args.mode == 'full':
            run_full_validation(args)
        elif args.mode == 'report':
            generate_report_only(args)
        
        print("\nâœ… éªŒè¯å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­éªŒè¯")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

def run_quick_validation(args):
    """è¿è¡Œå¿«é€ŸéªŒè¯"""
    print("ğŸƒ å¼€å§‹å¿«é€ŸéªŒè¯...")
    
    try:
        from quick_validator import QuickValidator
        
        validator = QuickValidator()
        
        # ä¿®æ”¹è¾“å‡ºè·¯å¾„
        validator.output_path = args.output
        os.makedirs(validator.output_path, exist_ok=True)
        
        # è¿è¡Œæµ‹è¯•
        results = validator.run_quick_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = validator.generate_quick_report()
        
        print(f"\nğŸ“Š å¿«é€ŸéªŒè¯ç»“æœ:")
        print(f"- æ€»æµ‹è¯•æ•°: {len(results)}")
        print(f"- æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
        print(f"- å¹³å‡å¾—åˆ†: {report['summary']['average_score']:.1f}/100")
        print(f"- æŠ¥å‘Šä¿å­˜è‡³: {validator.output_path}")
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥å¿«é€ŸéªŒè¯å™¨: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…")

def run_full_validation(args):
    """è¿è¡Œå®Œæ•´éªŒè¯"""
    print("ğŸ” å¼€å§‹å®Œæ•´éªŒè¯...")
    
    try:
        from validation_framework import ValidationFramework
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if not os.path.exists(args.config):
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            print("ä½¿ç”¨é»˜è®¤é…ç½®...")
        
        validator = ValidationFramework(args.config)
        
        # ä¿®æ”¹è¾“å‡ºè·¯å¾„
        validator.config['output_path'] = args.output
        os.makedirs(validator.config['output_path'], exist_ok=True)
        
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        print("ğŸ“ è¿è¡Œæµ‹è¯•ç”¨ä¾‹...")
        results = validator.run_all_tests()
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        print("ğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        report = validator.generate_evaluation_report()
        
        # ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœä¸ç¦ç”¨ï¼‰
        if not args.no_charts:
            print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            validator.visualize_results()
        
        print(f"\nğŸ“Š å®Œæ•´éªŒè¯ç»“æœ:")
        print(f"- æ€»æµ‹è¯•æ•°: {len(results)}")
        if report:
            print(f"- æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
            print(f"- å¹³å‡å¾—åˆ†: {report['summary']['average_score']:.1f}/100")
        print(f"- æŠ¥å‘Šä¿å­˜è‡³: {validator.config['output_path']}")
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥å®Œæ•´éªŒè¯æ¡†æ¶: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
    except Exception as e:
        print(f"âŒ å®Œæ•´éªŒè¯å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

def generate_report_only(args):
    """ä»…ç”ŸæˆæŠ¥å‘Š"""
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Šæ¨¡å¼...")
    
    if not args.input:
        print("âŒ æŠ¥å‘Šæ¨¡å¼éœ€è¦æŒ‡å®šè¾“å…¥ç›®å½• (--input)")
        return
    
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return
    
    try:
        # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
        results_file = None
        for filename in ['evaluation_report.json', 'quick_validation_report.json', 'intermediate_results.json']:
            filepath = os.path.join(args.input, filename)
            if os.path.exists(filepath):
                results_file = filepath
                break
        
        if not results_file:
            print(f"âŒ åœ¨ {args.input} ä¸­æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            return
        
        print(f"ğŸ“‚ è¯»å–ç»“æœæ–‡ä»¶: {results_file}")
        
        with open(results_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output, exist_ok=True)
        
        # ç”Ÿæˆæ–°çš„æŠ¥å‘Š
        if 'detailed_results' in data:
            results = data['detailed_results']
        elif 'test_results' in data:
            results = data['test_results']
        else:
            print("âŒ æ— æ³•ä»ç»“æœæ–‡ä»¶ä¸­æå–æµ‹è¯•ç»“æœ")
            return
        
        # ä½¿ç”¨è¯„ä¼°æŒ‡æ ‡æ¨¡å—ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        from evaluation_metrics import EvaluationMetrics
        
        evaluator = EvaluationMetrics()
        summary = evaluator.generate_performance_summary(results)
        
        # ä¿å­˜æ–°æŠ¥å‘Š
        report_file = os.path.join(args.output, 'regenerated_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è§†åŒ–
        if not args.no_charts:
            chart_file = os.path.join(args.output, 'metrics_visualization.png')
            evaluator.visualize_metrics(results, chart_file)
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆè‡³: {args.output}")
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    required_packages = [
        'pandas', 'numpy', 'matplotlib', 'seaborn', 
        'scikit-learn', 'yaml', 'jupyter_client'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    main()